# Assessment Framework - QA Automation Courses
## Comprehensive Assessment Strategy for Both Courses

**Last Updated:** November 24, 2025
**Version:** 1.0

---

## Table of Contents

1. [Overview](#overview)
2. [MentorMate QA Automation Course Assessment](#mentormate-qa-automation-course-assessment)
3. [QA-Automation-with-AI Course Assessment](#qa-automation-with-ai-course-assessment)
4. [Shared Assessment Tools](#shared-assessment-tools)
5. [Grading Guidelines](#grading-guidelines)
6. [Instructor Resources](#instructor-resources)

---

## Overview

This document outlines the complete assessment strategy for both QA automation courses. Each course has a tailored assessment approach that aligns with its specific learning objectives, student level, and delivery format.

### Philosophy

Our assessment strategy is based on:
- **Practical Application** - Focus on hands-on skills over theoretical knowledge
- **Progressive Complexity** - Assessments increase in difficulty throughout the course
- **Real-World Relevance** - Projects mirror actual job responsibilities
- **Continuous Feedback** - Multiple checkpoints for student growth
- **Fair and Transparent** - Clear rubrics and expectations

---

## MentorMate QA Automation Course Assessment

**Course Type:** Traditional + Optional AI Enhancement
**Duration:** 4 weeks (intensive Sunday sessions)
**Target Audience:** Beginners to QA automation
**Assessment Style:** Practical, hands-on, project-based

### Assessment Components

| Component | Weight | Description | Timing |
|-----------|--------|-------------|--------|
| **Weekly Quizzes** | 15% | Knowledge checks after each Sunday | End of each week |
| **Weekly Labs/Exercises** | 25% | Hands-on practice assignments | During/after class |
| **Participation & Engagement** | 10% | Class interaction, peer help | Throughout |
| **Final Capstone Project** | 40% | Comprehensive automation framework | Weeks 3-4 |
| **Presentation** | 10% | Project demonstration | Week 4 |

**Total:** 100%

### Detailed Breakdown

#### 1. Weekly Quizzes (15% total - 3.75% each)

**Purpose:** Validate understanding of core concepts

**Format:**
- 15-20 multiple choice and short answer questions
- 20-30 minutes to complete
- Open book/notes
- Minimum passing: 70%
- Can retake once if needed

**Week 1 Quiz Topics:**
- QA fundamentals and SDLC
- Testing types and pyramid
- Playwright basics
- Git fundamentals
- Environment setup

**Week 2 Quiz Topics:**
- JavaScript essentials
- CSS selectors and XPath
- Playwright API
- Page Object Model concepts
- Test structure

**Week 3 Quiz Topics:**
- Advanced Playwright features
- API testing with Postman
- Test organization
- Data-driven testing
- Assertions

**Week 4 Quiz Topics:**
- CI/CD concepts
- GitHub Actions
- Best practices
- Troubleshooting
- Performance testing basics

#### 2. Weekly Labs/Exercises (25% total - 6.25% each)

**Purpose:** Build practical skills through hands-on coding

**Format:**
- Completed during class (50%) and homework (50%)
- Auto-graded where possible
- Manual review for code quality
- Submission via GitHub

**Week 1 Exercises:**
- ✅ Set up development environment (Pass/Fail)
- ✅ Create first GitHub repository
- ✅ Write and run first Playwright test
- ✅ Git commits and push
**Grading Criteria:** Completion (5 pts) + Code quality (1.25 pts)

**Week 2 Exercises:**
- ✅ Write 5 UI tests with different selectors
- ✅ Implement basic Page Object Model
- ✅ Handle dynamic elements
- ✅ Create test data files
**Grading Criteria:** Functionality (4 pts) + Code quality (2.25 pts)

**Week 3 Exercises:**
- ✅ Build complete POM framework
- ✅ Create 10 API tests in Postman
- ✅ Data-driven test suite
- ✅ Export Postman collection
**Grading Criteria:** Functionality (4 pts) + Organization (2.25 pts)

**Week 4 Exercises:**
- ✅ Set up GitHub Actions workflow
- ✅ Configure test reports
- ✅ Add pre-commit hooks
- ✅ Implement test parallelization
**Grading Criteria:** Working CI/CD (5 pts) + Configuration (1.25 pts)

#### 3. Participation & Engagement (10%)

**Purpose:** Encourage active learning and collaboration

**Evaluation Criteria:**
- Attends all Sunday sessions (4 pts)
- Actively participates in discussions (2 pts)
- Helps peers on Slack/during class (2 pts)
- Asks thoughtful questions (2 pts)

**Tracking:**
- Instructor observation
- Slack activity logs
- Peer feedback (optional)

#### 4. Final Capstone Project (40%)

**Purpose:** Demonstrate mastery of all course concepts

**Details:** See [FINAL-PROJECT.md](MentorMate-QA-Automation/FINAL-PROJECT.md) for complete specifications

**Grading Rubric:**

| Category | Points | Criteria |
|----------|--------|----------|
| **UI Test Suite** | 10 pts | 15+ quality tests, POM implementation |
| **API Test Suite** | 6 pts | 10+ API tests, proper validation |
| **Code Quality** | 8 pts | Clean code, structure, naming |
| **CI/CD Pipeline** | 6 pts | Working GitHub Actions, reports |
| **Documentation** | 6 pts | README, test plan, architecture |
| **Presentation** | 4 pts | Demo, explanation, Q&A |

**Submission Requirements:**
- GitHub repository with all code
- Complete documentation
- Working CI/CD pipeline
- 10-15 minute presentation (live or recorded)
- Due: End of Week 4 (can extend to Week 5)

#### 5. Final Presentation (10%)

**Purpose:** Demonstrate communication and technical understanding

**Format:**
- 10-15 minutes (live or recorded video)
- Must cover: architecture, tests, CI/CD, challenges
- Q&A session (live presentations only)

**Rubric:**

| Criteria | Points |
|----------|--------|
| Clear introduction and overview | 2 |
| Effective test demonstration | 3 |
| Explains technical decisions | 2 |
| Shows CI/CD pipeline | 2 |
| Professional delivery | 1 |

### Grading Scale

| Grade | Percentage | Description |
|-------|------------|-------------|
| **A** | 90-100% | Exceeds expectations, advanced implementation |
| **B** | 80-89% | Meets all expectations, solid implementation |
| **C** | 70-79% | Meets most expectations, basic implementation |
| **D** | 60-69% | Below expectations, incomplete work |
| **F** | Below 60% | Does not meet minimum standards |

**Pass Requirement:** 70% overall

### Retake Policy

- Quizzes: One retake per quiz allowed
- Labs: Can resubmit within 1 week for partial credit
- Final Project: One revision allowed within 2 weeks

---

## QA-Automation-with-AI Course Assessment

**Course Type:** AI-First Automation
**Duration:** 8-12 weeks (self-paced)
**Target Audience:** QA engineers with basic automation experience
**Assessment Style:** Project-based, AI-assisted, portfolio-driven

### Assessment Components

| Component | Weight | Description | Frequency |
|-----------|--------|-------------|-----------|
| **Module Exercises** | 30% | Hands-on AI-assisted labs | 12 modules |
| **Module Quizzes** | 20% | Knowledge validation | 12 modules |
| **Milestone Projects** | 20% | Progressive deliverables | 3 checkpoints |
| **Final Capstone Project** | 30% | Comprehensive AI-automated testing | End of course |

**Total:** 100%

### Detailed Breakdown

#### 1. Module Exercises (30% total - 2.5% each)

**Purpose:** Practice AI-assisted test development skills

**Format:**
- Hands-on coding with AI tools
- Submit code + AI interaction logs
- Must demonstrate AI usage
- GitHub submissions

**Module-by-Module Breakdown:**

**Module 1: Introduction (2.5 pts)**
- Set up Claude Code
- Configure CLAUDE.md file
- Complete 3 AI-assisted interactions
- Reflection on AI capabilities

**Module 2: Context Engineering (2.5 pts)**
- Create comprehensive CLAUDE.md
- Test with 5 different prompts
- Iterate based on responses
- Document improvements

**Module 3: Private Repos (2.5 pts)**
- Clone private repository
- Document architecture with AI
- Generate README
- Create navigation guide

**Module 4: Documentation (2.5 pts)**
- Generate API documentation
- Create setup guide
- Write architecture overview
- Validate accuracy

**Module 5: Test Planning (2.5 pts)**
- AI-generated test strategy
- Test case generation
- Coverage matrix
- Prioritization document

**Module 6: Test Implementation (2.5 pts)**
- Generate 5 unit tests
- Create 3 integration tests
- Build 2 E2E tests
- Review and improve

**Module 7: Validation (2.5 pts)**
- Review AI-generated tests
- Apply quality checklist
- Improve test coverage
- Fix identified issues

**Module 8: Agentic Patterns (2.5 pts)**
- Implement prompt chaining
- Apply reflection pattern
- Use RAG for test knowledge
- MCP integration

**Module 9: CI/CD Integration (2.5 pts)**
- Set up GitHub Actions
- AI-assisted workflow
- Automated reporting
- Log analysis

**Module 10: Final Project Planning (2.5 pts)**
- Project proposal
- Test strategy
- Timeline
- AI tool selection

**Module 11: Infrastructure (2.5 pts)**
- Dockerize tests
- Docker Compose setup
- CI container configuration
- Documentation

**Module 12: Advanced Playwright (2.5 pts)**
- Network interception
- Visual regression tests
- Global auth patterns
- Advanced debugging

#### 2. Module Quizzes (20% total - 1.67% each)

**Purpose:** Validate understanding of AI-assisted testing concepts

**Format:**
- 10-15 questions per quiz
- Mix of multiple choice and scenario-based
- Must demonstrate understanding of AI capabilities AND limitations
- Passing: 70%
- Unlimited attempts (with delay)

**Sample Question Types:**

*Multiple Choice:*
```
Q: When should you use Claude Opus vs. Haiku?
A) Opus for simple tasks, Haiku for complex reasoning
B) Haiku for simple tasks, Opus for complex reasoning
C) Always use Opus for best results
D) Model choice doesn't matter
Answer: B
```

*Scenario-Based:*
```
Q: You ask AI to generate tests for a login form. The tests pass
but don't cover error cases. What should you do?
A) Accept the tests as-is
B) Manually add error cases without AI
C) Refine prompt to explicitly request error cases
D) This proves AI isn't useful
Answer: C
```

*AI Limitation Recognition:*
```
Q: Which task is AI LEAST reliable for?
A) Generating boilerplate test code
B) Explaining complex algorithms
C) Making architectural decisions without context
D) Suggesting test cases
Answer: C
```

#### 3. Milestone Projects (20% total)

**Purpose:** Progressive skill demonstration with checkpoints

**Milestone 1: Documentation & Planning (7% - Week 4)**
- Choose real codebase
- Generate comprehensive documentation
- Create test strategy
- Submit for feedback
**Deliverables:**
- Repository with documentation
- Test plan document
- AI interaction log
- Reflection on process

**Milestone 2: Test Implementation (7% - Week 8)**
- Implement 20+ tests
- Use AI for generation
- Apply validation checklist
- Document improvements made
**Deliverables:**
- Test suite code
- Before/after AI improvements
- Coverage report
- Quality analysis

**Milestone 3: CI/CD Integration (6% - Week 11)**
- Working CI/CD pipeline
- Automated reporting
- AI-driven analysis
- Performance optimization
**Deliverables:**
- GitHub Actions workflows
- Test reports
- Performance metrics
- Documentation

**Grading Rubric (Each Milestone):**

| Criteria | Excellent (90-100%) | Good (80-89%) | Satisfactory (70-79%) | Needs Work (<70%) |
|----------|---------------------|---------------|------------------------|-------------------|
| **AI Usage** | Sophisticated prompts, optimal tool usage | Effective AI usage | Basic AI usage | Minimal/ineffective AI use |
| **Code Quality** | Clean, maintainable, well-documented | Good structure, clear code | Functional but needs improvement | Poor quality or incomplete |
| **Documentation** | Comprehensive, clear, professional | Complete, mostly clear | Basic, some gaps | Inadequate or missing |
| **Deliverables** | All present, high quality | All present, good quality | Most present, acceptable quality | Missing or poor quality |

#### 4. Final Capstone Project (30%)

**Purpose:** Demonstrate mastery of AI-assisted test automation

**Requirements:**
- Real-world codebase (can be open source)
- Complete test strategy developed with AI
- 50+ automated tests (unit, integration, E2E)
- CI/CD pipeline with AI analysis
- Comprehensive documentation
- Presentation/demo

**Grading Rubric:**

| Category | Points | Excellent (90-100%) | Good (80-89%) | Satisfactory (70-79%) |
|----------|--------|---------------------|---------------|------------------------|
| **AI Tool Mastery** | 8 | Expert use of AI tools, optimal prompts | Effective AI usage | Basic AI usage |
| **Test Quality** | 8 | Comprehensive, well-designed tests | Good coverage, quality tests | Adequate test coverage |
| **Code Architecture** | 6 | Exceptional structure, patterns | Good architecture | Acceptable structure |
| **CI/CD Integration** | 4 | Advanced pipeline, AI analysis | Working CI/CD | Basic CI/CD |
| **Documentation** | 4 | Exceptional docs, clear process | Complete documentation | Adequate documentation |

**Submission Requirements:**
- GitHub repository (public or private with access)
- Video demonstration (15-20 minutes)
- AI interaction logs (selected highlights)
- Reflection document (2-3 pages)
- Due: End of Week 12 (flexible based on pace)

### Grading Scale

| Grade | Percentage | Description |
|-------|------------|-------------|
| **A** | 90-100% | Expert-level AI-assisted automation |
| **B** | 80-89% | Proficient AI-assisted automation |
| **C** | 70-79% | Competent AI-assisted automation |
| **F** | Below 70% | Does not meet minimum standards |

**Pass Requirement:** 70% overall + Final project submission

### AI Usage Policy

**Required:**
- All exercises must demonstrate AI tool usage
- Document prompts and iterations
- Explain AI-generated code
- Validate all AI outputs

**Prohibited:**
- Blindly accepting AI code without understanding
- Copy-paste without review
- Using AI for quizzes (knowledge checks)
- Claiming AI work as fully original

---

## Shared Assessment Tools

### 1. Code Quality Checklist

Use for all coding assignments:

```markdown
## Code Quality Review
- [ ] Code runs without errors
- [ ] Tests pass consistently
- [ ] Follows naming conventions
- [ ] No hardcoded values
- [ ] Proper error handling
- [ ] Meaningful comments
- [ ] Clean git history
- [ ] No sensitive data
```

### 2. Test Quality Rubric

| Criteria | Excellent | Good | Needs Improvement |
|----------|-----------|------|-------------------|
| **Coverage** | >90% critical paths | 70-90% coverage | <70% coverage |
| **Independence** | All tests isolated | Most tests isolated | Tests have dependencies |
| **Assertions** | Specific, meaningful | Adequate assertions | Generic or missing |
| **Maintainability** | Easy to update | Moderately maintainable | Difficult to maintain |
| **Documentation** | Well-documented | Basic documentation | Poor/no documentation |

### 3. Presentation Evaluation

| Criteria | Points | Description |
|----------|--------|-------------|
| **Clarity** | 0-3 | Clear communication, logical flow |
| **Demonstration** | 0-3 | Effective demo of features |
| **Technical Depth** | 0-2 | Shows understanding of concepts |
| **Q&A** | 0-2 | Answers questions confidently |

### 4. Peer Review Template (Optional)

Students can optionally review each other's work:

```markdown
## Peer Review Form

**Reviewer:** [Name]
**Reviewed:** [Name]
**Project:** [Project Name]

### What I Liked (3 things):
1.
2.
3.

### Suggestions for Improvement (3 things):
1.
2.
3.

### Questions I Have:
1.
2.

### Overall Impression:
[1-2 paragraphs]

### Rating (Optional):
Would you hire this person based on this project? [ ] Yes [ ] No [ ] Maybe
```

---

## Grading Guidelines

### For Instructors

#### General Principles

1. **Be Consistent**
   - Use rubrics for all grading
   - Apply same standards to all students
   - Document grading decisions

2. **Provide Feedback**
   - Specific, actionable feedback
   - Highlight strengths
   - Suggest improvements
   - Return within 5 business days

3. **Grade Fairly**
   - Focus on learning objectives
   - Consider effort and growth
   - Allow for revisions when appropriate

4. **Be Transparent**
   - Share rubrics in advance
   - Explain grading criteria
   - Make yourself available for questions

#### Time Estimates for Grading

| Item | Time per Student | Notes |
|------|------------------|-------|
| Weekly Quiz | 5 minutes | Mostly auto-graded |
| Weekly Exercise | 15-20 minutes | Code review required |
| Milestone Project | 30-45 minutes | Detailed review |
| Final Project | 60-90 minutes | Comprehensive evaluation |
| Presentation | 15-30 minutes | Watch + evaluate |

**Total per student (MentorMate):** ~6-8 hours
**Total per student (AI Course):** ~8-10 hours

#### Handling Edge Cases

**Late Submissions:**
- First offense: Warning, no penalty
- Second offense: 5% penalty per day
- After 1 week: 50% max score
- After 2 weeks: 0 points

**Academic Integrity:**
- First offense: Zero on assignment + warning
- Second offense: Fail course + report
- Plagiarism: Immediate fail + report

**Technical Issues:**
- Legitimate issues: Grant extension
- Require proof (screenshots, error logs)
- Set new reasonable deadline

**Exceptional Work:**
- Award bonus points
- Feature in course materials
- Offer recommendation letter

---

## Instructor Resources

### 1. Assessment Schedule Template

**MentorMate Course (4 Weeks):**

```
Week 1:
- Sunday: Teach Module 1
- Mon-Sat: Students complete exercises
- Sunday (next): Grade exercises + quiz

Week 2:
- Sunday: Teach Module 2, Return Week 1 grades
- Mon-Sat: Students complete exercises
- Sunday (next): Grade exercises + quiz

Week 3:
- Sunday: Teach Module 3, Return Week 2 grades
- Mon-Sat: Students start final project
- Sunday (next): Grade exercises + quiz

Week 4:
- Sunday: Teach Module 4, Return Week 3 grades
- Mon-Sat: Students finish final project
- Week 5: Grade final projects + presentations
```

**AI Course (12 Weeks):**

```
Weekly:
- Students submit module exercises
- Grade exercises within 3 days
- Quizzes auto-graded immediately

Milestones:
- Week 4: Grade Milestone 1 (3 days)
- Week 8: Grade Milestone 2 (3 days)
- Week 11: Grade Milestone 3 (3 days)

Final:
- Week 12-13: Grade final projects (1 week)
- Week 13: Return all grades
```

### 2. Grading Tools

**Recommended Tools:**
- **GitHub Classroom** - Assignment management
- **Rubric.io** - Rubric-based grading
- **CodeGrade** - Automated code review
- **Google Sheets** - Grade tracking
- **Loom** - Video feedback

### 3. Common Grading Mistakes

❌ **Don't:**
- Grade without rubric
- Apply different standards
- Delay feedback
- Grade while tired/rushed
- Focus only on negatives

✅ **Do:**
- Use rubrics consistently
- Provide specific feedback
- Return grades promptly
- Take breaks between batches
- Balance critique with praise

### 4. Sample Feedback Templates

**Positive Feedback:**
```
Great work on this assignment! Your [specific strength] demonstrates
strong understanding of [concept]. I especially liked [specific example].
For your next project, consider [one improvement]. Overall excellent work!

Grade: [X]/[Y]
```

**Constructive Feedback:**
```
Thank you for your submission. You've made good progress on [aspect].
To strengthen your work, focus on:
1. [Specific improvement with example]
2. [Specific improvement with example]
3. [Specific improvement with example]

I'd be happy to discuss these during office hours. Keep up the effort!

Grade: [X]/[Y]
```

**Needs Significant Work:**
```
Thank you for submitting. I can see you put in effort, but this assignment
needs significant improvement to meet the requirements:

Required Changes:
- [Must do item 1]
- [Must do item 2]
- [Must do item 3]

I strongly encourage you to:
- Attend office hours
- Review [specific module]
- Resubmit within 1 week for partial credit

Let's schedule a time to discuss.

Grade: [X]/[Y]
```

---

## Student Resources

### Self-Assessment Checklists

**Before Submitting Any Assignment:**
- [ ] All requirements met
- [ ] Code runs without errors
- [ ] Tests pass
- [ ] Documentation complete
- [ ] No sensitive data
- [ ] Followed naming conventions
- [ ] Git history is clean
- [ ] Reviewed rubric
- [ ] Spell-checked all docs
- [ ] Tested on clean environment

**Before Final Project Submission:**
- [ ] All components complete
- [ ] CI/CD pipeline works
- [ ] README is comprehensive
- [ ] All tests pass
- [ ] Presentation prepared
- [ ] Asked for peer review
- [ ] Double-checked rubric
- [ ] Submission form filled
- [ ] Proud of the work

---

## Appendix: Assessment Philosophy

### Why This Approach?

Our assessment framework is designed based on these principles:

1. **Authentic Assessment**
   - Tasks mirror real-world work
   - Portfolio-worthy deliverables
   - Industry-relevant skills

2. **Growth Mindset**
   - Focus on learning, not just grades
   - Allow revisions and retakes
   - Recognize improvement

3. **Continuous Feedback**
   - Regular checkpoints
   - Quick turnaround on grades
   - Actionable suggestions

4. **Fair and Equitable**
   - Clear rubrics
   - Consistent standards
   - Support for all learners

5. **Preparation for Employment**
   - Builds job-ready portfolio
   - Practices presentation skills
   - Demonstrates competence

### Success Metrics

We measure success by:
- Student pass rate (target: >85%)
- Project quality (average score >80%)
- Student satisfaction (target: 4.5/5)
- Job placement rate (track alumni)
- Employer feedback (informal)

---

## Version History

- **v1.0** (Nov 24, 2025) - Initial assessment framework
  - MentorMate course assessment structure
  - AI course assessment structure
  - Shared tools and guidelines
  - Instructor resources

---

## Feedback & Improvement

This framework is living and will evolve based on:
- Student feedback
- Instructor experience
- Industry changes
- Tool updates

**Submit feedback to:** [Instructor email or form]

---

*Assessment Framework created for QA Automation Courses*
*© 2025 - All assessment materials proprietary and confidential*
