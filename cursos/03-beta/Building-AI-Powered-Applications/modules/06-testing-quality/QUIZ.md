# Module 6 Quiz: Testing & Quality Assurance

**Instructions**: Answer all questions. Select the best answer for multiple choice questions. For scenario questions, provide brief explanations.

**Passing Score**: 70% (14 out of 20 points)

**Time Limit**: 25 minutes

---

## Section 1: Multiple Choice (10 questions, 1 point each)

### Question 1
Why is traditional exact-match testing insufficient for AI applications?

A) AI applications are too complex to test
B) AI outputs are non-deterministic and vary between runs
C) Testing frameworks don't support AI code
D) AI applications don't have bugs

### Question 2
What is the PRIMARY reason for mocking LLM calls in unit tests?

A) To make tests run faster and avoid API costs
B) To test error handling only
C) Because real API calls are forbidden in tests
D) To increase code coverage metrics

### Question 3
Which metric measures the longest common subsequence between generated and reference text?

A) BLEU
B) ROUGE-1
C) ROUGE-L
D) Perplexity

### Question 4
What does BLEU score primarily measure?

A) Semantic similarity using embeddings
B) N-gram overlap between candidate and reference
C) Response latency and performance
D) Cost per API call

### Question 5
In A/B testing, why is consistent hashing important for user assignment?

A) It makes the code run faster
B) It ensures the same user always gets the same variant
C) It reduces memory usage
D) It improves statistical significance

### Question 6
What is the correct approach when testing AI output quality?

A) Assert exact string equality with expected output
B) Test for structural correctness, semantic similarity, and constraint satisfaction
C) Skip testing since AI outputs are unpredictable
D) Only test in production with real users

### Question 7
What is a "golden dataset" in regression testing?

A) A dataset of expensive premium examples
B) Curated test cases with verified correct outputs for baseline comparison
C) The largest possible test dataset
D) A dataset generated by the AI itself

### Question 8
Which quality gate metric would MOST likely block a deployment?

A) Documentation completeness below 50%
B) Test pass rate of 75% when threshold is 95%
C) Response time of 1.5s when limit is 2s
D) Code coverage of 85%

### Question 9
When should you use semantic similarity instead of ROUGE/BLEU?

A) When testing translation accuracy
B) When the meaning matters more than exact words
C) When you need faster test execution
D) When testing structured data output

### Question 10
What is the purpose of mutation testing in AI applications?

A) To change the AI model parameters
B) To verify tests actually detect when code is broken
C) To generate new test cases automatically
D) To optimize model performance

---

## Section 2: True/False (5 questions, 1 point each)

### Question 11
True or False: A test that asserts `expect(result).toBeDefined()` is sufficient to verify an AI response is correct.

### Question 12
True or False: Mocking should be applied where the dependency is imported, not where it's defined.

### Question 13
True or False: 100% code coverage guarantees that all AI outputs are being properly validated.

### Question 14
True or False: A/B tests should use random assignment for each request to ensure fairness.

### Question 15
True or False: Quality gates should automatically block deployments when thresholds are not met.

---

## Section 3: Scenario-Based Questions (5 questions, 1 point each)

### Question 16
**Scenario**: You're writing unit tests for a summarization service. The tests pass locally but fail in CI because the OpenAI API returns slightly different responses.

**Question**: What is the BEST solution?

A) Add retry logic to the tests until they pass
B) Mock the OpenAI API calls to return deterministic responses
C) Skip the tests in CI and only run them locally
D) Increase the test timeout to handle API latency

### Question 17
**Scenario**: Your A/B test shows GPT-4 has 15% better quality scores than GPT-3.5, but the p-value is 0.12.

**Question**: What should you conclude?

A) GPT-4 is definitively better; roll it out immediately
B) The results are not statistically significant; gather more data
C) The p-value is close enough; deploy GPT-4
D) The test is flawed and should be discarded

### Question 18
**Scenario**: Your regression tests show ROUGE-L dropped from 0.45 to 0.38 after a prompt change.

**Question**: What is the MOST appropriate response?

A) Ignore it since the change is less than 20%
B) Investigate the cause and determine if quality actually degraded
C) Automatically rollback the prompt change
D) Update the baseline to 0.38 so tests pass

### Question 19
**Scenario**: You need to test that your API correctly handles rate limit errors from OpenAI.

**Question**: What is the BEST testing approach?

A) Wait for a real rate limit error to occur in production
B) Mock the API to raise a RateLimitError and verify the handling logic
C) Send thousands of requests to trigger a real rate limit
D) Skip testing since rate limits are rare

### Question 20
**Scenario**: Your quality gate checks pass rate (98%), latency (1.2s), and ROUGE score (0.42). Thresholds are: pass rate > 95%, latency < 2s, ROUGE > 0.3.

**Question**: Should the deployment proceed?

A) Yes, all metrics meet their thresholds
B) No, the ROUGE score is too low
C) No, the latency is concerning
D) Need more information about error rate

---

## Answer Key

### Section 1: Multiple Choice

1. **B** - AI outputs are non-deterministic and vary between runs
   - *Explanation: LLMs use temperature and sampling, producing different outputs for the same input. Traditional exact-match tests would fail randomly.*

2. **A** - To make tests run faster and avoid API costs
   - *Explanation: Mocking allows fast, free, deterministic tests. Real API calls are slow, expensive, and introduce flakiness.*

3. **C** - ROUGE-L
   - *Explanation: ROUGE-L measures the Longest Common Subsequence between texts, making it good for capturing sentence-level similarity.*

4. **B** - N-gram overlap between candidate and reference
   - *Explanation: BLEU calculates precision of n-grams (1-gram through 4-gram) in the candidate that appear in the reference.*

5. **B** - It ensures the same user always gets the same variant
   - *Explanation: Consistent hashing guarantees deterministic assignment, so users have a consistent experience and results are measurable.*

6. **B** - Test for structural correctness, semantic similarity, and constraint satisfaction
   - *Explanation: AI testing requires checking format, meaning, and constraints rather than exact output matching.*

7. **B** - Curated test cases with verified correct outputs for baseline comparison
   - *Explanation: Golden datasets contain human-verified examples used to detect quality regressions over time.*

8. **B** - Test pass rate of 75% when threshold is 95%
   - *Explanation: Being 20 percentage points below the pass rate threshold is a clear quality gate failure that should block deployment.*

9. **B** - When the meaning matters more than exact words
   - *Explanation: Semantic similarity via embeddings captures meaning even when different words are used, unlike n-gram based metrics.*

10. **B** - To verify tests actually detect when code is broken
    - *Explanation: Mutation testing introduces small bugs (mutants) to check if tests fail. Surviving mutants indicate weak tests.*

### Section 2: True/False

11. **False** - `toBeDefined()` only checks the value isn't undefined; it doesn't verify the content is correct, complete, or meaningful.

12. **True** - Python's mock.patch should target where the object is used/imported, not where it's originally defined, for the mock to take effect.

13. **False** - Coverage only measures code execution, not assertion quality. You can have 100% coverage with tests that assert nothing meaningful.

14. **False** - Random assignment per request would give inconsistent experiences. Consistent hashing ensures the same user always gets the same variant.

15. **True** - Automated quality gates provide objective enforcement of standards and prevent human error or pressure from bypassing checks.

### Section 3: Scenario-Based

16. **B** - Mock the OpenAI API calls to return deterministic responses
    - *Explanation: Mocking eliminates external dependencies, making tests fast, free, and deterministic. This is the standard practice for unit testing.*

17. **B** - The results are not statistically significant; gather more data
    - *Explanation: A p-value of 0.12 exceeds the typical 0.05 threshold, meaning there's a 12% chance the difference is due to random variation.*

18. **B** - Investigate the cause and determine if quality actually degraded
    - *Explanation: A 7-point drop in ROUGE-L is notable. Investigate whether this reflects real quality loss or if the metric doesn't capture the improvement.*

19. **B** - Mock the API to raise a RateLimitError and verify the handling logic
    - *Explanation: Mocking allows you to test error handling paths reliably without depending on external conditions.*

20. **A** - Yes, all metrics meet their thresholds
    - *Explanation: Pass rate (98% > 95%), latency (1.2s < 2s), and ROUGE (0.42 > 0.3) all pass. The deployment should proceed.*

---

## Scoring Guide

| Score | Grade | Performance Level |
|-------|-------|-------------------|
| 18-20 | A | Excellent - Strong grasp of AI testing concepts |
| 16-17 | B | Good - Solid understanding with minor gaps |
| 14-15 | C | Satisfactory - Meets minimum requirements |
| Below 14 | F | Review module content and retake quiz |

---

## Review Recommendations

**If you scored below 70%**, review these sections:

- Questions 1-3 wrong: Review "Why Testing AI Applications Is Different" and "Unit Testing AI Components"
- Questions 4-5 wrong: Review "Evaluation Metrics" and "A/B Testing Frameworks"
- Questions 6-7 wrong: Review "Regression Testing" and golden dataset concepts
- Questions 8-10 wrong: Review "Quality Gates & Monitoring"
- Scenario questions wrong: Review the complete module and work through the exercises

**Next Steps:**
- Complete Module 6 Exercise if not done
- Review any incorrect answers with the README explanations
- Proceed to the Final Project to apply all concepts

---

*Quiz 6 of 6 | Building AI-Powered Applications | 20 points total*
