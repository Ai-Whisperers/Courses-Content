# =============================================================================
# üß™ TESTING RULES - REGLAS DE TESTING Y CALIDAD
# Basado en: Google Testing, TDD, BDD, Microsoft Test Standards
# Proyecto: FPUNA AI Education - Lead Architect Testing Standards
# =============================================================================

meta:
  version: "1.0.0"
  last_updated: "2025-01-30"
  maintainer: "FPUNA AI Education Team"
  scope: ["testing", "tdd", "quality-assurance"]

# =============================================================================
# üéØ PHILOSOPH√çA DE TESTING
# =============================================================================

philosophy:
  # Google: Testing is part of development, not separate
  development_driven:
    principle: "Tests are first-class citizens"
    rules:
      - "Write tests before or alongside code (TDD preferred)"
      - "Untested code is incomplete code"
      - "Tests are documentation"
      - "Maintain tests with same care as production code"
  
  # Testing pyramid
  pyramid:
    unit_tests:
      percentage: 70
      description: "Fast, isolated, cheap"
      count: "Many (hundreds/thousands)"
    
    integration_tests:
      percentage: 20
      description: "Components working together"
      count: "Medium (tens/hundreds)"
    
    e2e_tests:
      percentage: 10
      description: "Full user flows"
      count: "Few (tens)"
    
    # What NOT to do
    anti_patterns:
      - "Ice cream cone (many E2E, few unit)"
      - "Hourglass (many unit/E2E, few integration)"

# =============================================================================
# üìä COBERTURA REQUERIDA
# =============================================================================

coverage:
  # Thresholds (strict for production, relaxed for education)
  thresholds:
    overall:
      required: 70
      target: 85
      excellent: 90
    
    by_type:
      lines: {required: 70, target: 85}
      functions: {required: 80, target: 90}
      branches: {required: 60, target: 75}
      
    by_module:
      critical: {required: 90, target: 95}  # Auth, payments
      standard: {required: 70, target: 85}  # Business logic
      simple: {required: 50, target: 70}    # Config, utils
  
  # Enforcement
  enforcement:
    ci_failure: true
    pr_blocking: true
    exemptions: "Requires lead architect approval"
    
  # What counts toward coverage
  in_scope:
    - "Production code"
    - "Public APIs"
    - "Business logic"
  
  out_of_scope:
    - "Generated code"
    - "Migration scripts"
    - "__main__ blocks"
    - "Debug/development utilities"

# =============================================================================
# üß¨ TIPOS DE TESTS
# =============================================================================

test_types:
  # Unit Tests
  unit:
    definition: "Test one unit (function/class) in isolation"
    characteristics:
      - "Fast (< 10ms each)"
      - "Isolated (no DB, no network)"
      - "Deterministic (same result every time)"
      - "Single reason to fail"
    
    scope:
      - "Public functions"
      - "Class methods"
      - "Business logic"
    
    mocking:
      - "All external dependencies"
      - "Database"
      - "External APIs"
      - "File system"
      - "Time"
    
    naming: "test_<function_name>_<scenario>_<expected_result>"
    examples:
      - "test_calculate_total_with_valid_items_returns_sum"
      - "test_authenticate_user_with_wrong_password_raises_error"
      
  # Integration Tests
  integration:
    definition: "Test multiple components working together"
    characteristics:
      - "Medium speed (10-100ms)"
      - "Real dependencies (test DB)"
      - "May be flaky (handle gracefully)"
    
    scope:
      - "Repository + Database"
      - "Service + Repository"
      - "API + Service + Repository"
    
    database:
      strategy: "Test database per test/run"
      setup: "Schema creation + fixtures"
      teardown: "Cleanup or rollback"
      isolation: "Each test in transaction"
    
    naming: "test_<component1>_with_<component2>_<scenario>"
    examples:
      - "test_user_service_with_repository_creates_user"
      - "test_api_with_service_returns_correct_response"
      
  # End-to-End Tests
  e2e:
    definition: "Test complete user flows"
    characteristics:
      - "Slow (seconds)"
      - "Full stack"
      - "Most realistic"
      - "Flaky by nature (handle with retries)"
    
    scope:
      - "User registration flow"
      - "Payment process"
      - "Search and filtering"
    
    tools: ["Playwright", "Selenium", "Cypress"]
    
    environment: "Production-like staging"
    
    data: "Test data, reset between runs"
    
  # Contract Tests
  contract:
    definition: "Test API contracts between services"
    when_to_use: "Microservices with external consumers"
    tools: ["Pact", "Spring Cloud Contract"]
    
  # Performance Tests
  performance:
    types:
      - "Load testing (normal load)"
      - "Stress testing (peak load)"
      - "Spike testing (sudden increases)"
      - "Endurance testing (extended periods)"
    
    metrics:
      - "Response time (p50, p95, p99)"
      - "Throughput (requests/sec)"
      - "Error rate"
      - "Resource utilization"
    
    targets:
      response_time_p95_ms: 500
      error_rate_max: 0.1
      throughput_rps: 1000

# =============================================================================
# üìù ESTRUCTURA DE TESTS
# =============================================================================

test_structure:
  # Arrange-Act-Assert (AAA)
  aaa:
    description: "Every test follows AAA pattern"
    arrange: "Set up test data and dependencies"
    act: "Execute the code being tested"
    assert: "Verify the results"
    
    example: |
      def test_user_can_change_password():
          # Arrange
          user = create_user(password="old_pass")
          
          # Act
          result = user.change_password("old_pass", "new_pass")
          
          # Assert
          assert result is True
          assert user.verify_password("new_pass") is True
  
  # Given-When-Then (BDD style)
  gherkin:
    description: "Alternative structure for behavior tests"
    given: "Preconditions and context"
    when: "Action performed"
    then: "Expected outcomes"
    
    example: |
      def test_user_login_with_valid_credentials():
          # Given
          user = User(email="test@example.com", password="secret")
          user.save()
          
          # When
          result = authenticate(email="test@example.com", password="secret")
          
          # Then
          assert result.is_success is True
          assert result.user.email == "test@example.com"
  
  # Test fixtures
  fixtures:
    scope:
      - "function: Fresh for each test (default)"
      - "class: Shared across class methods"
      - "module: Shared across module"
      - "session: Shared across test run"
    
    naming: "descriptive, lowercase_with_underscores"
    examples:
      - "user_fixture"
      - "authenticated_client"
      - "test_database"
    
    factory_pattern: "Use factories, not fixtures for complex data"

# =============================================================================
# üé≠ MOCKS Y STUBS
# =============================================================================

mocking:
  # When to mock
  when_to_mock:
    - "External services/APIs"
    - "Database (in unit tests)"
    - "File system"
    - "Random number generators"
    - "Time/clock"
    - "Email/SMS services"
  
  # When NOT to mock
  when_not_to_mock:
    - "Domain logic (test the real thing)"
    - "Value objects"
    - "Simple data structures"
    - "In integration tests (use real dependencies)"
  
  # Mock types
  types:
    dummy:
      description: "Objects passed around but never used"
      example: "Unused constructor parameter"
    
    fake:
      description: "Working implementation but not production"
      example: "In-memory database"
    
    stub:
      description: "Canned answers to calls"
      example: "Repository returning hardcoded data"
    
    spy:
      description: "Records calls for verification"
      example: "Email service spy"
    
    mock:
      description: "Pre-programmed with expectations"
      example: "Mocked API client with expected calls"
  
  # Best practices
  best_practices:
    - "Mock at boundaries"
    - "Verify behavior, not implementation"
    - "Don't over-specify (allow some flexibility)"
    - "Keep mocks simple"
    - "Reset mocks between tests"
    - "Don't mock what you don't own (use adapter)"

# =============================================================================
# üîÑ TDD - TEST DRIVEN DEVELOPMENT
# =============================================================================

tdd:
  # Red-Green-Refactor cycle
  cycle:
    red:
      - "Write a failing test"
      - "Run and confirm it fails"
      - "Minimal code to make it compile"
    
    green:
      - "Write minimal code to pass test"
      - "Can be ugly/hardcoded"
      - "All tests must pass"
    
    refactor:
      - "Clean up the code"
      - "Keep tests passing"
      - "Improve design"
  
  # Rules
  rules:
    - "Write no production code without failing test"
    - "Write only enough test to fail (compilation is failure)"
    - "Write only enough code to pass test"
  
  # When to use
  when_to_use:
    - "New features with clear requirements"
    - "Bug fixes (reproduce with test first)"
    - "Refactoring (ensure no regression)"
    - "Algorithm implementation"
  
  # When NOT to use
  when_not_to_use:
    - "Spikes/exploration"
    - "UI development (use BDD instead)"
    - "When requirements unclear"

# =============================================================================
# üèóÔ∏è TEST DATA
# =============================================================================

test_data:
  # Factory pattern (preferred)
  factories:
    library: "factory_boy"
    pattern: "Define blueprints, create variations"
    example: |
      class UserFactory(factory.Factory):
          class Meta:
              model = User
          
          email = factory.Faker("email")
          name = factory.Faker("name")
          is_active = True
      
      # Usage
      user = UserFactory()  # Random but valid
      admin = UserFactory(role="admin")
  
  # Fixtures
  fixtures:
    location: "tests/fixtures/"
    formats: ["JSON", "YAML", "CSV", "SQL"]
    naming: "<entity>_<scenario>.json"
    examples:
      - "users_default.json"
      - "products_out_of_stock.json"
  
  # Test data principles
  principles:
    - "Valid by default (happy path)"
    - "Explicit edge cases"
    - "No shared mutable state"
    - "Clean up after tests"
    - "Use realistic but fake data (no real PII)"
  
  # Fakers
  faker:
    use_for: "Generating realistic test data"
    locales: ["en_US", "es_ES", "pt_BR"]
    seed: "Set for reproducibility"

# =============================================================================
# ‚ö° EJECUCI√ìN DE TESTS
# =============================================================================

test_execution:
  # Speed targets
  speed:
    unit_test_max_ms: 10
    integration_test_max_ms: 100
    e2e_test_max_ms: 5000
    total_suite_max_seconds: 300
  
  # Parallelization
  parallel:
    unit_tests: true
    integration_tests: false  # DB conflicts
    workers: "auto (CPU count)"
  
  # Test selection
  selection:
    failed_first: true
    smart_order: "Failed, new, slow, fast"
    file_watching: "pytest-watch or similar"
    
  # Retries
  retries:
    flaky_tests: 2
    e2e_tests: 1
    never_for: "unit tests (should be deterministic)"

# =============================================================================
# üìà METRICS DE CALIDAD DE TESTS
# =============================================================================

test_quality:
  # Good test indicators
  good_test:
    - "Fast (< 100ms)"
    - "Isolated (no order dependencies)"
    - "Repeatable (same result every time)"
    - "Self-validating (assertions)"
    - "Timely (written with code)"
  
  # Bad test indicators
  bad_test:
    - "Slow (> 1s)"
    - "Brittle (breaks on unrelated changes)"
    - "Flaky (sometimes fails)"
    - "No assertions (smoke test only)"
    - "Testing implementation details"
  
  # Test smells
  smells:
    - "Mystery guest (unclear data)"
    - "Conditional logic in tests"
    - "Multiple assertions (may indicate multiple tests)"
    - "Catching exceptions (use pytest.raises)"
    - "External resources (should be mocked)"
    - "Sleeps/waits (use synchronization)"

# =============================================================================
# üîß CONFIGURACI√ìN DE PYTEST
# =============================================================================

pytest:
  # Configuration file
  config_file: "pyproject.toml or pytest.ini"
  
  # Key settings
  settings:
    testpaths: ["tests"]
    python_files: ["test_*.py", "*_test.py"]
    python_functions: ["test_*"]
    addopts: "-v --tb=short --strict-markers"
    
  # Markers
  markers:
    - "unit: Unit tests (fast, isolated)"
    - "integration: Integration tests (uses DB/API)"
    - "e2e: End-to-end tests (full flow)"
    - "slow: Tests taking > 1s"
    - "flaky: Known flaky tests (retries enabled)"
    - "skip_ci: Skip in CI environment"
    - "security: Security-related tests"
    
  # Plugins
  plugins:
    - "pytest-cov (coverage)"
    - "pytest-asyncio (async tests)"
    - "pytest-mock (mocking)"
    - "pytest-xdist (parallel)"
    - "pytest-django (Django support)"
    - "pytest-postgresql (PostgreSQL)"

# =============================================================================
# ‚úÖ CHECKLIST DE TESTING
# =============================================================================

checklist:
  before_commit:
    - "[ ] All new code has tests"
    - "[ ] Tests pass locally"
    - "[ ] Coverage maintained or improved"
    - "[ ] No new flaky tests"
    - "[ ] Mock external dependencies"
    - "[ ] Test edge cases"
    - "[ ] Clear test names"
  
  code_review:
    - "[ ] Test coverage appropriate"
    - "[ ] Tests are readable"
    - "[ ] Edge cases covered"
    - "[ ] No test smells"
    - "[ ] Test data realistic"
    - "[ ] Assertions meaningful"
  
  before_release:
    - "[ ] All test suites pass"
    - "[ ] Coverage meets targets"
    - "[ ] No critical/high bugs"
    - "[ ] Performance tests pass"
    - "[ ] E2E tests pass in staging"
